---
title: "Modeling"
author: "Chris Hill"
format: 
  html:
    toc: true
    toc-title: "Contents"
    toc-depth: 3
editor: visual
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidymodels)
library(doFuture)

registerDoFuture()
plan(multisession)

set.seed(123)
```

# Introduction

The goal is to predict Diabetes status using the predictors described below. Two models will be tuned and compared, Regression Tree and Random Forest. Which ever model performs best using the log loss metric will be used in an API.

For this analysis, we focus on the following key variables:

- Response Variable: Diabetes_binary (No Diabetes, Prediabetes, Diabetes), indicating an individual's diabetes status.

Predictors:

- BMI (Body Mass Index): Numeric variable representing an individual's weight-to-height ratio.

- Sex: Binary categorical variable indicating gender (Female, Male).

- Age: Ordered factor capturing age groups from "18-24" to "80 or older."

- GenHlth (General Health): Ordered factor ranging from "Excellent" to "Poor."

- HeartDiseaseorAttack: Binary indicator of previous heart disease or heart attack (Yes, No).

## Data

Read in the data processed in our [EDA](EDA.html)

```{r}
#| label: data-processing

data <- readRDS("processed_data.rds")
```

## Data Splitting

The data is split into training (70%) and testing (30%) sets, stratified by our response Diabetes_binary

```{r}
#| label: data-split

split <- initial_split(data, prop = 0.7, strata = Diabetes_binary)
train_data <- training(split)
test_data <- testing(split)
```

## Metric

Log loss is our primary metric. Log loss will penalize based on the confidence of the prediction. Accuracy is also included for interest.

```{r}
#| label: metric

log_loss_metric <- metric_set(mn_log_loss, accuracy)
```

## Cross Validation

In cross validation, each fold is used once as a validation set, while the remaining are used for training.

```{r}
#| label: cross-validation

folds <- vfold_cv(train_data, v = 5, strata = Diabetes_binary)
```

# Classification Tree

## Recipe

- Defines the predictors and response variable.

- Encodes dummy variables for categorical predictors.

```{r}
#| label: tree-recipe

tree_recipe <- recipe(Diabetes_binary ~ BMI + Sex + Age + GenHlth + HeartDiseaseorAttack, 
                      data = train_data) |>
  step_dummy(all_nominal_predictors(), -all_outcomes())
```

To verify the dummy variables are created correctly, we prep and bake the recipe.

```{r}
#| label: tree-prep

prepared_recipe <- prep(tree_recipe)
baked <- bake(prepared_recipe, new_data = NULL)
names(baked)
```

## Model

```{r}
#| label: tree-model

tree_model <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

## Workflow

```{r}
#| label: tree-workflow

tree_workflow <- workflow() |>
  add_recipe(tree_recipe) |>
  add_model(tree_model)
```

## Tuning

The cost_complexity parameter is tuned to find the best model. A larger value of cost_complexity, the simpler the tree. 

```{r}
#| label: tree-tuning
#| cache: true
#| dependson: "tree-model"

tree_grid <- grid_regular(cost_complexity(range = c(-6, -2)), 
                          levels = 10)

tree_tuning <- tune_grid(
  tree_workflow,
  resamples = folds,
  grid = tree_grid,
  metrics = log_loss_metric
)
```

## Tuning Results Visualization

```{r}
#| label: tree-tuning-results

tree_results <- collect_metrics(tree_tuning)

tree_plot <- tree_results |> 
  ggplot(aes(x = cost_complexity, y = mean)) +
  geom_point(size = 3, aes(color = .metric)) +
  geom_line(aes(color = .metric)) +
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err, 
        color = .metric),
    width = 0.2
  ) +
  scale_x_log10() +  
  facet_wrap(~ .metric, scales = "free_y") +
  labs(
    title = "Tuning Metrics for Decision Tree",
    x = "Cost Complexity",
    y = "Mean Metric Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

print(tree_plot)
```

## Best Model

The best tree model is selected based on log_loss during CV and then fit to the full training set

```{r}
#| label: best-tree
#| cache: true
#| dependson: "tree-tuning"

best_tree <- tree_tuning |>
  select_best(metric = "mn_log_loss")
best_tree

final_tree_workflow <- finalize_workflow(tree_workflow, best_tree)

final_tree_model <- fit(final_tree_workflow, data = train_data)
```

# Random Forest

## Recipe

Same as the classification tree.

## Model

```{r}
#| label: rf-model

rf_model <- rand_forest(mtry = tune(), trees = 1000) |>
  set_engine("ranger") |>
  set_mode("classification")
```

## Workflow

```{r}
#| label: rf-workflow

rf_workflow <- workflow() |>
  add_recipe(tree_recipe) |>
  add_model(rf_model)
```

## Tuning

The mtry parameter is tuned, testing a range of predictors sampled at each split.

```{r}
#| label: rf-tuning
#| cache: true
#| dependson: ["rf-model", "rf-workflow", "tree-model"]

rf_grid <- grid_regular(mtry(range = c(1, 5)), levels = 5)

rf_tuning <- tune_grid(
  rf_workflow,
  resamples = folds,
  grid = rf_grid,
  metrics = log_loss_metric
)
```

## Best Model

The best random forest model is selected based on log_loss during CV and then fit to the full training set

```{r}
#| label: best-rf
#| cache: true
#| dependson: "rf-tuning"

best_rf <- rf_tuning |>
  select_best(metric = "mn_log_loss")
best_rf

final_rf_workflow <- finalize_workflow(rf_workflow, best_rf)

final_rf_model <- fit(final_rf_workflow, data = train_data)
```

## Tuning Results Visualization

```{r}
#| label: rf-tuning-results

rf_results <- collect_metrics(rf_tuning)

# Random Forest Plot
rf_plot <- rf_results |> 
  ggplot(aes(x = mtry, y = mean)) +
  geom_point(size = 3, aes(color = .metric)) +
  geom_line(aes(color = .metric)) +
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err, 
        color = .metric),
    width = 0.2
  ) +
  facet_wrap(~ .metric, scales = "free_y") + 
  labs(
    title = "Tuning Metrics for Random Forest",
    x = "Number of Predictors Sampled (mtry)",
    y = "Mean Metric Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none", 
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

print(rf_plot)
```

# Model Comparison

Both the best classification tree and random forest models are evaluated on the test set.

```{r}
#| label: model-comparison
#| cache: true
#| dependson: ["best-tree", "best-rf"]

tree_results <- final_tree_workflow |>
  last_fit(split, metrics = log_loss_metric) |>
  collect_metrics()

rf_results <- final_rf_workflow |>
  last_fit(split, metrics = log_loss_metric) |>
  collect_metrics()

list(
  Classification_Tree = tree_results,
  Random_Forest = rf_results
)
```

# Fit Best Model to Entire Dataset

The best model is chosen using the log loss metric and fit to the full dataset.

The full model is then saved for use in the API.

```{r}
#| label: fit-full
#| cache: true
#| dependson: "best-rf"

final_rf_model_full <- fit(final_rf_workflow, data = data)

saveRDS(final_rf_model_full, "final_rf_model.rds")
```

